{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiboliu/AIPI540-NLP/blob/main/AIPI540_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8AqHyf9usbr",
        "outputId": "0b36dfb6-f491-4529-b43f-ad750594da0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "import copy\n",
        "import time\n",
        "import csv\n",
        "import gensim\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5SS9KXC2dP-"
      },
      "outputs": [],
      "source": [
        "def unzip_file(file_name):\n",
        "    with ZipFile(file_name, 'r') as zf:\n",
        "        zf.extractall()\n",
        "        print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrOaTPvxu10G"
      },
      "source": [
        "Below are the functions for selecting words from the given tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "qutIy3uHLv1X"
      },
      "outputs": [],
      "source": [
        "def preprocess(sentence):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer() # lemmatize each word\n",
        "    # stemmer = PorterStemmer() # stem each word\n",
        "    sentence = sentence.lower() # to lower cases\n",
        "    tokenizer = RegexpTokenizer(r'\\w+') # tokenize and remove punctuations\n",
        "    words = tokenizer.tokenize(sentence)\n",
        "    results = []\n",
        "    for word in words:\n",
        "        if word in stop_words:\n",
        "            continue\n",
        "        lemmatized = lemmatizer.lemmatize(word)\n",
        "        # results.append(stemmer.stem(lemmatized))\n",
        "        results.append(lemmatized)\n",
        "\n",
        "    return results\n",
        "\n",
        "def select_words_for_each_sent(preprocessed, window_size):\n",
        "    G = nx.Graph()\n",
        "    for word in preprocessed:\n",
        "        G.add_node(word)\n",
        "\n",
        "    for i in range(len(preprocessed)):\n",
        "        for distance in range(1, window_size):\n",
        "            if i + distance >= len(preprocessed):\n",
        "                break\n",
        "            left = preprocessed[i]\n",
        "            right = preprocessed[i+distance]\n",
        "            if G.has_edge(left, right):\n",
        "                G[left][right]['weight'] += 1\n",
        "            else:\n",
        "                G.add_edge(preprocessed[i], preprocessed[i+distance], weight=1)\n",
        "    scores = nx.pagerank(G)\n",
        "    selected = [k for k in scores if scores[k] >= float(1.0 / len(scores))]\n",
        "    return \" \".join(selected)\n",
        "\n",
        "def select_words(file_name):\n",
        "    df = pd.DataFrame(columns=['textID', 'selected_text', 'sentiment'])\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        cnt = 0\n",
        "        for row in reader:\n",
        "            preprocessed = preprocess(row['text'])\n",
        "            selected_text = select_words_for_each_sent(preprocessed, 2)\n",
        "            if not selected_text:\n",
        "                continue\n",
        "            cnt += 1\n",
        "            df = pd.concat([df, pd.DataFrame({'textID': [row['textID']], 'selected_text': [selected_text], 'sentiment': [row['sentiment']]})])\n",
        "        df.index = pd.RangeIndex(start=0,stop=cnt, step=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpaGg8C-IOSs"
      },
      "source": [
        "This code block is for processing the selected keywords to find its sentiment. TextClassification in both DL and non DL will be used for that purpose. The DL approach is PyTorch embeddingbag and linear layer. The non DL is Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpINmvDlr_ej"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch import nn\n",
        "from torchtext import data\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEdsbHrxOyA0"
      },
      "outputs": [],
      "source": [
        "def build_datasets(df):\n",
        "    train_iter = [(label, text) for label, text in zip(df['sentiment'].to_list(), df['selected_text'].to_list())]\n",
        "\n",
        "    train_dataset = to_map_style_dataset(train_iter)\n",
        "\n",
        "    num_train = int(len(train_dataset) * 0.8)\n",
        "    num_val = len(train_dataset) - num_train\n",
        "    train_dataset, val_dataset = random_split(train_dataset, [num_train, num_val])\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def yield_tokens(data_iter, tokenizer):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "def build_vocab(data_iter):\n",
        "    tokenizer = get_tokenizer('spacy')\n",
        "    vocab = build_vocab_from_iterator(yield_tokens(data_iter, tokenizer), specials=[\"<unk>\"])\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "    return vocab, tokenizer\n",
        "\n",
        "def collate_fn(batch, tokenizer, vocab):\n",
        "    mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "    label_pipeline = lambda x: mapping[x]\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (label, text) in batch:\n",
        "        # print(f'text is {text}')\n",
        "        label_list.append(label_pipeline(label))\n",
        "        # print(f'label is {label}')\n",
        "        # print(len(label_list))\n",
        "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
        "        # print(f'processed_text is {processed_text}')\n",
        "        # print(f'processed_text.size(0) is {processed_text.size(0)}')\n",
        "        text_list.append(processed_text)\n",
        "        offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    # print(f'label_list length is {label_list.size(0)}')\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    # offsets = torch.tensor(offsets)\n",
        "    # print(f\"Processed text lengths: {[text.size(0) for text in text_list]}\")\n",
        "    text_list = torch.cat(text_list)\n",
        "    # print(f'max offset: {offsets[-1]}, textlist length {text_list.size(0)}')\n",
        "    # print(f\"Offsets: {offsets}\")\n",
        "\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "def get_dataloader(dataset, batch_size, vocab, tokenizer):\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn = lambda x: collate_fn(x, tokenizer, vocab))\n",
        "\n",
        "def build_dataloaders(df):\n",
        "    train_dataset, val_dataset = build_datasets(df)\n",
        "    vocab, tokenizer = build_vocab(train_dataset)\n",
        "    batch_size = 128\n",
        "    train_dataloader = get_dataloader(train_dataset, batch_size, vocab, tokenizer)\n",
        "    val_dataloader = get_dataloader(val_dataset, batch_size, vocab, tokenizer)\n",
        "    dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n",
        "    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "    return dataloaders, dataset_sizes, vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "1gP8nJzR1Tkl"
      },
      "outputs": [],
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\",sparse=True)\n",
        "        # Fully connected final layer to convert embeddings to output predictions\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        # print(f'text is {text.shape}')\n",
        "        # print(f'offsets is {offsets.shape}')\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1SH9m-p4Sp3"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    model = model.to(device)\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for labels, text, offsets in dataloaders[phase]:\n",
        "                text = text.to(device)\n",
        "                labels = labels.to(device)\n",
        "                offsets = offsets.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model.forward(text, offsets)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * text.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'train':\n",
        "              scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def launch_training_dl(dataloaders, dataset_sizes, vocab, model_path):\n",
        "    num_classes = 3\n",
        "    vocab_size = len(vocab)\n",
        "    embed_dim = 64\n",
        "    model = TextClassificationModel(vocab_size, embed_dim, num_classes).to(device)\n",
        "    epochs = 100\n",
        "    lr = 5.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2.0, gamma=0.9)\n",
        "    model = train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=epochs)\n",
        "    torch.save(model.state_dict(), model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_features(data, ngram_range):\n",
        "    print(data.shape)\n",
        "    vec = TfidfVectorizer(ngram_range=ngram_range)\n",
        "    transformed = vec.fit_transform(data)\n",
        "    print(transformed.shape)\n",
        "    return transformed, vec\n",
        "\n",
        "def train_and_test(X, y):\n",
        "    num_folds = 5\n",
        "    model = LogisticRegression(solver='saga', max_iter=5000)\n",
        "    for i in range(num_folds):\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_val)\n",
        "        print(accuracy_score(y_val, preds))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "yaTQ_gapelbJ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB7pUt8o2l6L",
        "outputId": "e2c10691-d58f-4e4e-c235-8a2495506f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "unzip_file('tweet-sentiment-extraction.zip')\n",
        "df = select_words('train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats_df(df):\n",
        "    vals = []\n",
        "    for item in df['selected_text']:\n",
        "      vals.append(len(item.split()))\n",
        "    counts = np.array(vals)\n",
        "    return np.mean(counts), counts.max()"
      ],
      "metadata": {
        "id": "95JljN5ZXxMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWbleJcv_eN2",
        "outputId": "6447f5e1-5dae-4e2e-ce64-f1461293e3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "----------\n",
            "train Loss: 4.9848 Acc: 0.4773\n",
            "train Loss: 4.9848 Acc: 0.4773\n",
            "val Loss: 4.7073 Acc: 0.5257\n",
            "val Loss: 4.7073 Acc: 0.5257\n",
            "Epoch 2/100\n",
            "----------\n",
            "train Loss: 4.5703 Acc: 0.5419\n",
            "train Loss: 4.5703 Acc: 0.5419\n",
            "val Loss: 4.5990 Acc: 0.5545\n",
            "val Loss: 4.5990 Acc: 0.5545\n",
            "Epoch 3/100\n",
            "----------\n",
            "train Loss: 4.2905 Acc: 0.5849\n",
            "train Loss: 4.2905 Acc: 0.5849\n",
            "val Loss: 4.5830 Acc: 0.5562\n",
            "val Loss: 4.5830 Acc: 0.5562\n",
            "Epoch 4/100\n",
            "----------\n",
            "train Loss: 4.1102 Acc: 0.6106\n",
            "train Loss: 4.1102 Acc: 0.6106\n",
            "val Loss: 4.6588 Acc: 0.5596\n",
            "val Loss: 4.6588 Acc: 0.5596\n",
            "Epoch 5/100\n",
            "----------\n",
            "train Loss: 3.9522 Acc: 0.6365\n",
            "train Loss: 3.9522 Acc: 0.6365\n",
            "val Loss: 4.7191 Acc: 0.5604\n",
            "val Loss: 4.7191 Acc: 0.5604\n",
            "Epoch 6/100\n",
            "----------\n",
            "train Loss: 3.8172 Acc: 0.6532\n",
            "train Loss: 3.8172 Acc: 0.6532\n",
            "val Loss: 5.5951 Acc: 0.4923\n",
            "val Loss: 5.5951 Acc: 0.4923\n",
            "Epoch 7/100\n",
            "----------\n",
            "train Loss: 3.7017 Acc: 0.6657\n",
            "train Loss: 3.7017 Acc: 0.6657\n",
            "val Loss: 4.9061 Acc: 0.5567\n",
            "val Loss: 4.9061 Acc: 0.5567\n",
            "Epoch 8/100\n",
            "----------\n",
            "train Loss: 3.6006 Acc: 0.6794\n",
            "train Loss: 3.6006 Acc: 0.6794\n",
            "val Loss: 5.1017 Acc: 0.5534\n",
            "val Loss: 5.1017 Acc: 0.5534\n",
            "Epoch 9/100\n",
            "----------\n",
            "train Loss: 3.4900 Acc: 0.6916\n",
            "train Loss: 3.4900 Acc: 0.6916\n",
            "val Loss: 5.1432 Acc: 0.5463\n",
            "val Loss: 5.1432 Acc: 0.5463\n",
            "Epoch 10/100\n",
            "----------\n",
            "train Loss: 3.4076 Acc: 0.7014\n",
            "train Loss: 3.4076 Acc: 0.7014\n",
            "val Loss: 5.2027 Acc: 0.5458\n",
            "val Loss: 5.2027 Acc: 0.5458\n",
            "Epoch 11/100\n",
            "----------\n",
            "train Loss: 3.3161 Acc: 0.7146\n",
            "train Loss: 3.3161 Acc: 0.7146\n",
            "val Loss: 5.2917 Acc: 0.5555\n",
            "val Loss: 5.2917 Acc: 0.5555\n",
            "Epoch 12/100\n",
            "----------\n",
            "train Loss: 3.2607 Acc: 0.7199\n",
            "train Loss: 3.2607 Acc: 0.7199\n",
            "val Loss: 5.5866 Acc: 0.5430\n",
            "val Loss: 5.5866 Acc: 0.5430\n",
            "Epoch 13/100\n",
            "----------\n",
            "train Loss: 3.1717 Acc: 0.7315\n",
            "train Loss: 3.1717 Acc: 0.7315\n",
            "val Loss: 5.5726 Acc: 0.5462\n",
            "val Loss: 5.5726 Acc: 0.5462\n",
            "Epoch 14/100\n",
            "----------\n",
            "train Loss: 3.1210 Acc: 0.7373\n",
            "train Loss: 3.1210 Acc: 0.7373\n",
            "val Loss: 5.7404 Acc: 0.5434\n",
            "val Loss: 5.7404 Acc: 0.5434\n",
            "Epoch 15/100\n",
            "----------\n",
            "train Loss: 3.0448 Acc: 0.7463\n",
            "train Loss: 3.0448 Acc: 0.7463\n",
            "val Loss: 5.7659 Acc: 0.5429\n",
            "val Loss: 5.7659 Acc: 0.5429\n",
            "Epoch 16/100\n",
            "----------\n",
            "train Loss: 2.9956 Acc: 0.7546\n",
            "train Loss: 2.9956 Acc: 0.7546\n",
            "val Loss: 5.8621 Acc: 0.5427\n",
            "val Loss: 5.8621 Acc: 0.5427\n",
            "Epoch 17/100\n",
            "----------\n",
            "train Loss: 2.9378 Acc: 0.7597\n",
            "train Loss: 2.9378 Acc: 0.7597\n",
            "val Loss: 5.9291 Acc: 0.5405\n",
            "val Loss: 5.9291 Acc: 0.5405\n",
            "Epoch 18/100\n",
            "----------\n",
            "train Loss: 2.8884 Acc: 0.7618\n",
            "train Loss: 2.8884 Acc: 0.7618\n",
            "val Loss: 6.4662 Acc: 0.5133\n",
            "val Loss: 6.4662 Acc: 0.5133\n",
            "Epoch 19/100\n",
            "----------\n",
            "train Loss: 2.8224 Acc: 0.7717\n",
            "train Loss: 2.8224 Acc: 0.7717\n",
            "val Loss: 6.1813 Acc: 0.5430\n",
            "val Loss: 6.1813 Acc: 0.5430\n",
            "Epoch 20/100\n",
            "----------\n",
            "train Loss: 2.8012 Acc: 0.7748\n",
            "train Loss: 2.8012 Acc: 0.7748\n",
            "val Loss: 6.3355 Acc: 0.5361\n",
            "val Loss: 6.3355 Acc: 0.5361\n",
            "Epoch 21/100\n",
            "----------\n",
            "train Loss: 2.7464 Acc: 0.7775\n",
            "train Loss: 2.7464 Acc: 0.7775\n",
            "val Loss: 6.2070 Acc: 0.5456\n",
            "val Loss: 6.2070 Acc: 0.5456\n",
            "Epoch 22/100\n",
            "----------\n",
            "train Loss: 2.7224 Acc: 0.7814\n",
            "train Loss: 2.7224 Acc: 0.7814\n",
            "val Loss: 6.5432 Acc: 0.5255\n",
            "val Loss: 6.5432 Acc: 0.5255\n",
            "Epoch 23/100\n",
            "----------\n",
            "train Loss: 2.6679 Acc: 0.7853\n",
            "train Loss: 2.6679 Acc: 0.7853\n",
            "val Loss: 6.5771 Acc: 0.5330\n",
            "val Loss: 6.5771 Acc: 0.5330\n",
            "Epoch 24/100\n",
            "----------\n",
            "train Loss: 2.6465 Acc: 0.7910\n",
            "train Loss: 2.6465 Acc: 0.7910\n",
            "val Loss: 6.7716 Acc: 0.5223\n",
            "val Loss: 6.7716 Acc: 0.5223\n",
            "Epoch 25/100\n",
            "----------\n",
            "train Loss: 2.5926 Acc: 0.7955\n",
            "train Loss: 2.5926 Acc: 0.7955\n",
            "val Loss: 6.8286 Acc: 0.5301\n",
            "val Loss: 6.8286 Acc: 0.5301\n",
            "Epoch 26/100\n",
            "----------\n",
            "train Loss: 2.5776 Acc: 0.7963\n",
            "train Loss: 2.5776 Acc: 0.7963\n",
            "val Loss: 6.8757 Acc: 0.5254\n",
            "val Loss: 6.8757 Acc: 0.5254\n",
            "Epoch 27/100\n",
            "----------\n",
            "train Loss: 2.5176 Acc: 0.8006\n",
            "train Loss: 2.5176 Acc: 0.8006\n",
            "val Loss: 6.8527 Acc: 0.5224\n",
            "val Loss: 6.8527 Acc: 0.5224\n",
            "Epoch 28/100\n",
            "----------\n",
            "train Loss: 2.5039 Acc: 0.8034\n",
            "train Loss: 2.5039 Acc: 0.8034\n",
            "val Loss: 7.0176 Acc: 0.5288\n",
            "val Loss: 7.0176 Acc: 0.5288\n",
            "Epoch 29/100\n",
            "----------\n",
            "train Loss: 2.4670 Acc: 0.8058\n",
            "train Loss: 2.4670 Acc: 0.8058\n",
            "val Loss: 6.9639 Acc: 0.5296\n",
            "val Loss: 6.9639 Acc: 0.5296\n",
            "Epoch 30/100\n",
            "----------\n",
            "train Loss: 2.4476 Acc: 0.8062\n",
            "train Loss: 2.4476 Acc: 0.8062\n",
            "val Loss: 7.1362 Acc: 0.5221\n",
            "val Loss: 7.1362 Acc: 0.5221\n",
            "Epoch 31/100\n",
            "----------\n",
            "train Loss: 2.4104 Acc: 0.8115\n",
            "train Loss: 2.4104 Acc: 0.8115\n",
            "val Loss: 7.2092 Acc: 0.5221\n",
            "val Loss: 7.2092 Acc: 0.5221\n",
            "Epoch 32/100\n",
            "----------\n",
            "train Loss: 2.3987 Acc: 0.8135\n",
            "train Loss: 2.3987 Acc: 0.8135\n",
            "val Loss: 7.2652 Acc: 0.5294\n",
            "val Loss: 7.2652 Acc: 0.5294\n",
            "Epoch 33/100\n",
            "----------\n",
            "train Loss: 2.3630 Acc: 0.8165\n",
            "train Loss: 2.3630 Acc: 0.8165\n",
            "val Loss: 7.3230 Acc: 0.5252\n",
            "val Loss: 7.3230 Acc: 0.5252\n",
            "Epoch 34/100\n",
            "----------\n",
            "train Loss: 2.3488 Acc: 0.8152\n",
            "train Loss: 2.3488 Acc: 0.8152\n",
            "val Loss: 7.2368 Acc: 0.5343\n",
            "val Loss: 7.2368 Acc: 0.5343\n",
            "Epoch 35/100\n",
            "----------\n",
            "train Loss: 2.3198 Acc: 0.8204\n",
            "train Loss: 2.3198 Acc: 0.8204\n",
            "val Loss: 7.3769 Acc: 0.5348\n",
            "val Loss: 7.3769 Acc: 0.5348\n",
            "Epoch 36/100\n",
            "----------\n",
            "train Loss: 2.3063 Acc: 0.8190\n",
            "train Loss: 2.3063 Acc: 0.8190\n",
            "val Loss: 7.3908 Acc: 0.5308\n",
            "val Loss: 7.3908 Acc: 0.5308\n",
            "Epoch 37/100\n",
            "----------\n",
            "train Loss: 2.2804 Acc: 0.8244\n",
            "train Loss: 2.2804 Acc: 0.8244\n",
            "val Loss: 7.4074 Acc: 0.5305\n",
            "val Loss: 7.4074 Acc: 0.5305\n",
            "Epoch 38/100\n",
            "----------\n",
            "train Loss: 2.2632 Acc: 0.8271\n",
            "train Loss: 2.2632 Acc: 0.8271\n",
            "val Loss: 7.5738 Acc: 0.5279\n",
            "val Loss: 7.5738 Acc: 0.5279\n",
            "Epoch 39/100\n",
            "----------\n",
            "train Loss: 2.2378 Acc: 0.8261\n",
            "train Loss: 2.2378 Acc: 0.8261\n",
            "val Loss: 7.5842 Acc: 0.5283\n",
            "val Loss: 7.5842 Acc: 0.5283\n",
            "Epoch 40/100\n",
            "----------\n",
            "train Loss: 2.2261 Acc: 0.8281\n",
            "train Loss: 2.2261 Acc: 0.8281\n",
            "val Loss: 7.6322 Acc: 0.5292\n",
            "val Loss: 7.6322 Acc: 0.5292\n",
            "Epoch 41/100\n",
            "----------\n",
            "train Loss: 2.1965 Acc: 0.8302\n",
            "train Loss: 2.1965 Acc: 0.8302\n",
            "val Loss: 7.7046 Acc: 0.5279\n",
            "val Loss: 7.7046 Acc: 0.5279\n",
            "Epoch 42/100\n",
            "----------\n",
            "train Loss: 2.1881 Acc: 0.8322\n",
            "train Loss: 2.1881 Acc: 0.8322\n",
            "val Loss: 7.7745 Acc: 0.5186\n",
            "val Loss: 7.7745 Acc: 0.5186\n",
            "Epoch 43/100\n",
            "----------\n",
            "train Loss: 2.1652 Acc: 0.8328\n",
            "train Loss: 2.1652 Acc: 0.8328\n",
            "val Loss: 7.7868 Acc: 0.5223\n",
            "val Loss: 7.7868 Acc: 0.5223\n",
            "Epoch 44/100\n",
            "----------\n",
            "train Loss: 2.1551 Acc: 0.8331\n",
            "train Loss: 2.1551 Acc: 0.8331\n",
            "val Loss: 7.8631 Acc: 0.5193\n",
            "val Loss: 7.8631 Acc: 0.5193\n",
            "Epoch 45/100\n",
            "----------\n",
            "train Loss: 2.1412 Acc: 0.8335\n",
            "train Loss: 2.1412 Acc: 0.8335\n",
            "val Loss: 7.8226 Acc: 0.5268\n",
            "val Loss: 7.8226 Acc: 0.5268\n",
            "Epoch 46/100\n",
            "----------\n",
            "train Loss: 2.1292 Acc: 0.8347\n",
            "train Loss: 2.1292 Acc: 0.8347\n",
            "val Loss: 7.9953 Acc: 0.5208\n",
            "val Loss: 7.9953 Acc: 0.5208\n",
            "Epoch 47/100\n",
            "----------\n",
            "train Loss: 2.1045 Acc: 0.8379\n",
            "train Loss: 2.1045 Acc: 0.8379\n",
            "val Loss: 7.9712 Acc: 0.5303\n",
            "val Loss: 7.9712 Acc: 0.5303\n",
            "Epoch 48/100\n",
            "----------\n",
            "train Loss: 2.1016 Acc: 0.8381\n",
            "train Loss: 2.1016 Acc: 0.8381\n",
            "val Loss: 7.9887 Acc: 0.5272\n",
            "val Loss: 7.9887 Acc: 0.5272\n",
            "Epoch 49/100\n",
            "----------\n",
            "train Loss: 2.0827 Acc: 0.8399\n",
            "train Loss: 2.0827 Acc: 0.8399\n",
            "val Loss: 8.0705 Acc: 0.5263\n",
            "val Loss: 8.0705 Acc: 0.5263\n",
            "Epoch 50/100\n",
            "----------\n",
            "train Loss: 2.0797 Acc: 0.8394\n",
            "train Loss: 2.0797 Acc: 0.8394\n",
            "val Loss: 8.0184 Acc: 0.5292\n",
            "val Loss: 8.0184 Acc: 0.5292\n",
            "Epoch 51/100\n",
            "----------\n",
            "train Loss: 2.0580 Acc: 0.8436\n",
            "train Loss: 2.0580 Acc: 0.8436\n",
            "val Loss: 8.0958 Acc: 0.5292\n",
            "val Loss: 8.0958 Acc: 0.5292\n",
            "Epoch 52/100\n",
            "----------\n",
            "train Loss: 2.0494 Acc: 0.8424\n",
            "train Loss: 2.0494 Acc: 0.8424\n",
            "val Loss: 8.1554 Acc: 0.5265\n",
            "val Loss: 8.1554 Acc: 0.5265\n",
            "Epoch 53/100\n",
            "----------\n",
            "train Loss: 2.0339 Acc: 0.8437\n",
            "train Loss: 2.0339 Acc: 0.8437\n",
            "val Loss: 8.1435 Acc: 0.5279\n",
            "val Loss: 8.1435 Acc: 0.5279\n",
            "Epoch 54/100\n",
            "----------\n",
            "train Loss: 2.0279 Acc: 0.8442\n",
            "train Loss: 2.0279 Acc: 0.8442\n",
            "val Loss: 8.1577 Acc: 0.5303\n",
            "val Loss: 8.1577 Acc: 0.5303\n",
            "Epoch 55/100\n",
            "----------\n",
            "train Loss: 2.0164 Acc: 0.8450\n",
            "train Loss: 2.0164 Acc: 0.8450\n",
            "val Loss: 8.2672 Acc: 0.5226\n",
            "val Loss: 8.2672 Acc: 0.5226\n",
            "Epoch 56/100\n",
            "----------\n",
            "train Loss: 2.0098 Acc: 0.8449\n",
            "train Loss: 2.0098 Acc: 0.8449\n",
            "val Loss: 8.2516 Acc: 0.5268\n",
            "val Loss: 8.2516 Acc: 0.5268\n",
            "Epoch 57/100\n",
            "----------\n",
            "train Loss: 1.9958 Acc: 0.8460\n",
            "train Loss: 1.9958 Acc: 0.8460\n",
            "val Loss: 8.2868 Acc: 0.5235\n",
            "val Loss: 8.2868 Acc: 0.5235\n",
            "Epoch 58/100\n",
            "----------\n",
            "train Loss: 1.9940 Acc: 0.8462\n",
            "train Loss: 1.9940 Acc: 0.8462\n",
            "val Loss: 8.3014 Acc: 0.5274\n",
            "val Loss: 8.3014 Acc: 0.5274\n",
            "Epoch 59/100\n",
            "----------\n",
            "train Loss: 1.9825 Acc: 0.8476\n",
            "train Loss: 1.9825 Acc: 0.8476\n",
            "val Loss: 8.3503 Acc: 0.5254\n",
            "val Loss: 8.3503 Acc: 0.5254\n",
            "Epoch 60/100\n",
            "----------\n",
            "train Loss: 1.9783 Acc: 0.8492\n",
            "train Loss: 1.9783 Acc: 0.8492\n",
            "val Loss: 8.3421 Acc: 0.5303\n",
            "val Loss: 8.3421 Acc: 0.5303\n",
            "Epoch 61/100\n",
            "----------\n",
            "train Loss: 1.9670 Acc: 0.8482\n",
            "train Loss: 1.9670 Acc: 0.8482\n",
            "val Loss: 8.3626 Acc: 0.5283\n",
            "val Loss: 8.3626 Acc: 0.5283\n",
            "Epoch 62/100\n",
            "----------\n",
            "train Loss: 1.9617 Acc: 0.8504\n",
            "train Loss: 1.9617 Acc: 0.8504\n",
            "val Loss: 8.4049 Acc: 0.5279\n",
            "val Loss: 8.4049 Acc: 0.5279\n",
            "Epoch 63/100\n",
            "----------\n",
            "train Loss: 1.9519 Acc: 0.8492\n",
            "train Loss: 1.9519 Acc: 0.8492\n",
            "val Loss: 8.4082 Acc: 0.5283\n",
            "val Loss: 8.4082 Acc: 0.5283\n",
            "Epoch 64/100\n",
            "----------\n",
            "train Loss: 1.9467 Acc: 0.8506\n",
            "train Loss: 1.9467 Acc: 0.8506\n",
            "val Loss: 8.4332 Acc: 0.5257\n",
            "val Loss: 8.4332 Acc: 0.5257\n",
            "Epoch 65/100\n",
            "----------\n",
            "train Loss: 1.9385 Acc: 0.8513\n",
            "train Loss: 1.9385 Acc: 0.8513\n",
            "val Loss: 8.4659 Acc: 0.5279\n",
            "val Loss: 8.4659 Acc: 0.5279\n",
            "Epoch 66/100\n",
            "----------\n",
            "train Loss: 1.9410 Acc: 0.8515\n",
            "train Loss: 1.9410 Acc: 0.8515\n",
            "val Loss: 8.4759 Acc: 0.5243\n",
            "val Loss: 8.4759 Acc: 0.5243\n",
            "Epoch 67/100\n",
            "----------\n",
            "train Loss: 1.9297 Acc: 0.8519\n",
            "train Loss: 1.9297 Acc: 0.8519\n",
            "val Loss: 8.5230 Acc: 0.5283\n",
            "val Loss: 8.5230 Acc: 0.5283\n",
            "Epoch 68/100\n",
            "----------\n",
            "train Loss: 1.9246 Acc: 0.8525\n",
            "train Loss: 1.9246 Acc: 0.8525\n",
            "val Loss: 8.5181 Acc: 0.5265\n",
            "val Loss: 8.5181 Acc: 0.5265\n",
            "Epoch 69/100\n",
            "----------\n",
            "train Loss: 1.9190 Acc: 0.8523\n",
            "train Loss: 1.9190 Acc: 0.8523\n",
            "val Loss: 8.5418 Acc: 0.5310\n",
            "val Loss: 8.5418 Acc: 0.5310\n",
            "Epoch 70/100\n",
            "----------\n",
            "train Loss: 1.9175 Acc: 0.8526\n",
            "train Loss: 1.9175 Acc: 0.8526\n",
            "val Loss: 8.5421 Acc: 0.5292\n",
            "val Loss: 8.5421 Acc: 0.5292\n",
            "Epoch 71/100\n",
            "----------\n",
            "train Loss: 1.9088 Acc: 0.8522\n",
            "train Loss: 1.9088 Acc: 0.8522\n",
            "val Loss: 8.5756 Acc: 0.5281\n",
            "val Loss: 8.5756 Acc: 0.5281\n",
            "Epoch 72/100\n",
            "----------\n",
            "train Loss: 1.9072 Acc: 0.8539\n",
            "train Loss: 1.9072 Acc: 0.8539\n",
            "val Loss: 8.5840 Acc: 0.5272\n",
            "val Loss: 8.5840 Acc: 0.5272\n",
            "Epoch 73/100\n",
            "----------\n",
            "train Loss: 1.8999 Acc: 0.8552\n",
            "train Loss: 1.8999 Acc: 0.8552\n",
            "val Loss: 8.6156 Acc: 0.5275\n",
            "val Loss: 8.6156 Acc: 0.5275\n",
            "Epoch 74/100\n",
            "----------\n",
            "train Loss: 1.8959 Acc: 0.8550\n",
            "train Loss: 1.8959 Acc: 0.8550\n",
            "val Loss: 8.6163 Acc: 0.5259\n",
            "val Loss: 8.6163 Acc: 0.5259\n",
            "Epoch 75/100\n",
            "----------\n",
            "train Loss: 1.8907 Acc: 0.8555\n",
            "train Loss: 1.8907 Acc: 0.8555\n",
            "val Loss: 8.6097 Acc: 0.5290\n",
            "val Loss: 8.6097 Acc: 0.5290\n",
            "Epoch 76/100\n",
            "----------\n",
            "train Loss: 1.8885 Acc: 0.8554\n",
            "train Loss: 1.8885 Acc: 0.8554\n",
            "val Loss: 8.6278 Acc: 0.5281\n",
            "val Loss: 8.6278 Acc: 0.5281\n",
            "Epoch 77/100\n",
            "----------\n",
            "train Loss: 1.8819 Acc: 0.8564\n",
            "train Loss: 1.8819 Acc: 0.8564\n",
            "val Loss: 8.6345 Acc: 0.5265\n",
            "val Loss: 8.6345 Acc: 0.5265\n",
            "Epoch 78/100\n",
            "----------\n",
            "train Loss: 1.8809 Acc: 0.8564\n",
            "train Loss: 1.8809 Acc: 0.8564\n",
            "val Loss: 8.6419 Acc: 0.5275\n",
            "val Loss: 8.6419 Acc: 0.5275\n",
            "Epoch 79/100\n",
            "----------\n",
            "train Loss: 1.8779 Acc: 0.8564\n",
            "train Loss: 1.8779 Acc: 0.8564\n",
            "val Loss: 8.6370 Acc: 0.5297\n",
            "val Loss: 8.6370 Acc: 0.5297\n",
            "Epoch 80/100\n",
            "----------\n",
            "train Loss: 1.8764 Acc: 0.8560\n",
            "train Loss: 1.8764 Acc: 0.8560\n",
            "val Loss: 8.6853 Acc: 0.5288\n",
            "val Loss: 8.6853 Acc: 0.5288\n",
            "Epoch 81/100\n",
            "----------\n",
            "train Loss: 1.8709 Acc: 0.8581\n",
            "train Loss: 1.8709 Acc: 0.8581\n",
            "val Loss: 8.6757 Acc: 0.5286\n",
            "val Loss: 8.6757 Acc: 0.5286\n",
            "Epoch 82/100\n",
            "----------\n",
            "train Loss: 1.8696 Acc: 0.8568\n",
            "train Loss: 1.8696 Acc: 0.8568\n",
            "val Loss: 8.7051 Acc: 0.5274\n",
            "val Loss: 8.7051 Acc: 0.5274\n",
            "Epoch 83/100\n",
            "----------\n",
            "train Loss: 1.8671 Acc: 0.8579\n",
            "train Loss: 1.8671 Acc: 0.8579\n",
            "val Loss: 8.6956 Acc: 0.5270\n",
            "val Loss: 8.6956 Acc: 0.5270\n",
            "Epoch 84/100\n",
            "----------\n",
            "train Loss: 1.8647 Acc: 0.8558\n",
            "train Loss: 1.8647 Acc: 0.8558\n",
            "val Loss: 8.6901 Acc: 0.5290\n",
            "val Loss: 8.6901 Acc: 0.5290\n",
            "Epoch 85/100\n",
            "----------\n",
            "train Loss: 1.8631 Acc: 0.8583\n",
            "train Loss: 1.8631 Acc: 0.8583\n",
            "val Loss: 8.6952 Acc: 0.5296\n",
            "val Loss: 8.6952 Acc: 0.5296\n",
            "Epoch 86/100\n",
            "----------\n",
            "train Loss: 1.8611 Acc: 0.8583\n",
            "train Loss: 1.8611 Acc: 0.8583\n",
            "val Loss: 8.7114 Acc: 0.5277\n",
            "val Loss: 8.7114 Acc: 0.5277\n",
            "Epoch 87/100\n",
            "----------\n",
            "train Loss: 1.8562 Acc: 0.8592\n",
            "train Loss: 1.8562 Acc: 0.8592\n",
            "val Loss: 8.7208 Acc: 0.5279\n",
            "val Loss: 8.7208 Acc: 0.5279\n",
            "Epoch 88/100\n",
            "----------\n",
            "train Loss: 1.8551 Acc: 0.8581\n",
            "train Loss: 1.8551 Acc: 0.8581\n",
            "val Loss: 8.7375 Acc: 0.5288\n",
            "val Loss: 8.7375 Acc: 0.5288\n",
            "Epoch 89/100\n",
            "----------\n",
            "train Loss: 1.8513 Acc: 0.8580\n",
            "train Loss: 1.8513 Acc: 0.8580\n",
            "val Loss: 8.7337 Acc: 0.5288\n",
            "val Loss: 8.7337 Acc: 0.5288\n",
            "Epoch 90/100\n",
            "----------\n",
            "train Loss: 1.8518 Acc: 0.8585\n",
            "train Loss: 1.8518 Acc: 0.8585\n",
            "val Loss: 8.7483 Acc: 0.5281\n",
            "val Loss: 8.7483 Acc: 0.5281\n",
            "Epoch 91/100\n",
            "----------\n",
            "train Loss: 1.8494 Acc: 0.8598\n",
            "train Loss: 1.8494 Acc: 0.8598\n",
            "val Loss: 8.7398 Acc: 0.5277\n",
            "val Loss: 8.7398 Acc: 0.5277\n",
            "Epoch 92/100\n",
            "----------\n",
            "train Loss: 1.8481 Acc: 0.8589\n",
            "train Loss: 1.8481 Acc: 0.8589\n",
            "val Loss: 8.7477 Acc: 0.5277\n",
            "val Loss: 8.7477 Acc: 0.5277\n",
            "Epoch 93/100\n",
            "----------\n",
            "train Loss: 1.8464 Acc: 0.8590\n",
            "train Loss: 1.8464 Acc: 0.8590\n",
            "val Loss: 8.7758 Acc: 0.5290\n",
            "val Loss: 8.7758 Acc: 0.5290\n",
            "Epoch 94/100\n",
            "----------\n",
            "train Loss: 1.8463 Acc: 0.8587\n",
            "train Loss: 1.8463 Acc: 0.8587\n",
            "val Loss: 8.7503 Acc: 0.5285\n",
            "val Loss: 8.7503 Acc: 0.5285\n",
            "Epoch 95/100\n",
            "----------\n",
            "train Loss: 1.8422 Acc: 0.8598\n",
            "train Loss: 1.8422 Acc: 0.8598\n",
            "val Loss: 8.7642 Acc: 0.5290\n",
            "val Loss: 8.7642 Acc: 0.5290\n",
            "Epoch 96/100\n",
            "----------\n",
            "train Loss: 1.8406 Acc: 0.8604\n",
            "train Loss: 1.8406 Acc: 0.8604\n",
            "val Loss: 8.7752 Acc: 0.5283\n",
            "val Loss: 8.7752 Acc: 0.5283\n",
            "Epoch 97/100\n",
            "----------\n",
            "train Loss: 1.8415 Acc: 0.8598\n",
            "train Loss: 1.8415 Acc: 0.8598\n",
            "val Loss: 8.7785 Acc: 0.5285\n",
            "val Loss: 8.7785 Acc: 0.5285\n",
            "Epoch 98/100\n",
            "----------\n",
            "train Loss: 1.8380 Acc: 0.8601\n",
            "train Loss: 1.8380 Acc: 0.8601\n",
            "val Loss: 8.7863 Acc: 0.5292\n",
            "val Loss: 8.7863 Acc: 0.5292\n",
            "Epoch 99/100\n",
            "----------\n",
            "train Loss: 1.8387 Acc: 0.8604\n",
            "train Loss: 1.8387 Acc: 0.8604\n",
            "val Loss: 8.7789 Acc: 0.5275\n",
            "val Loss: 8.7789 Acc: 0.5275\n",
            "Epoch 100/100\n",
            "----------\n",
            "train Loss: 1.8387 Acc: 0.8609\n",
            "train Loss: 1.8387 Acc: 0.8609\n",
            "val Loss: 8.7889 Acc: 0.5277\n",
            "val Loss: 8.7889 Acc: 0.5277\n",
            "Training complete in 2m 45s\n",
            "Best val Acc: 0.560379\n"
          ]
        }
      ],
      "source": [
        "dataloaders, dataset_sizes, vocab = build_dataloaders(df)\n",
        "launch_training_dl(dataloaders, dataset_sizes, vocab, 'model-dl.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "fGWmKQWg2spk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b34bcda-fbb9-4769-9689-86ca2712130b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "(27408,)\n",
            "(27408, 368121)\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "(5482, 368121)\n",
            "0.5726012404232033\n"
          ]
        }
      ],
      "source": [
        "def launch_training_nondl(df, model_path):\n",
        "    avg, max = get_stats_df(df)\n",
        "    print(max)\n",
        "\n",
        "    X_train, vec = build_features(df['selected_text'], (1, max))\n",
        "    y_train = df['sentiment']\n",
        "    model = train_and_test(X_train, y_train)\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    return model, vec\n",
        "\n",
        "model, vec = launch_training_nondl(df, 'model-lr.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def serve_model(model_path, vec, input):\n",
        "    with open(model_path, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    preprocessed = preprocess(input)\n",
        "    words = select_words_for_each_sent(preprocessed, 2)\n",
        "    print(words)\n",
        "    print(df.head())\n",
        "    trans = vec.transform([words])\n",
        "    print(trans.shape)\n",
        "    features = trans.toarray()\n",
        "\n",
        "    print(model.predict(features))\n",
        "\n",
        "serve_model('model-lr.pkl', vec, 'that was a good game')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm-2mgRUVNfa",
        "outputId": "c9dad8be-fd3a-458d-f76a-25daa4031ccd"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good game\n",
            "       textID        selected_text sentiment\n",
            "0  cb774db0d1      responded going   neutral\n",
            "1  549e992a42         sad miss san  negative\n",
            "2  088c60f138         bos bullying  negative\n",
            "3  9642c003ef                leave  negative\n",
            "4  358bd9e861  put release already  negative\n",
            "(1, 368121)\n",
            "['positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def serve_model_dl(model_path, vocab, input):\n",
        "    mapping = {2: 'positive', 1: 'neutral', 0: 'negative'}\n",
        "    model = TextClassificationModel(len(vocab), 64, 3).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    preprocessed = preprocess(input)\n",
        "    words = select_words_for_each_sent(preprocessed, 2)\n",
        "    print(words)\n",
        "    nums = [vocab.get_stoi()[word] for word in words.split()]\n",
        "    print(nums)\n",
        "    input = torch.tensor(nums).unsqueeze(0).to(device)\n",
        "    output = model(text=input, offsets=None)\n",
        "    print(output)\n",
        "    print(mapping[output.argmax().item()])\n",
        "\n",
        "serve_model_dl('model-dl.pth', vocab, 'I am so happy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcItt3Ir0Lfv",
        "outputId": "eb3c44e5-3549-4a68-e3a1-4d1d157973dd"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy\n",
            "[37]\n",
            "tensor([[-5.1556, -1.0620,  7.2979]], grad_fn=<AddmmBackward0>)\n",
            "positive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}